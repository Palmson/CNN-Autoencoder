{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./public/dataset\"\n",
    "test_data_dir = \"./public/test_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "dataset = ImageFolder(data_dir, transform=data_transform)\n",
    "test_dataset = ImageFolder(test_data_dir, transform=data_transform)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_size = len(dataset) - 2000\n",
    "train_data, val_data = random_split(dataset, [train_size, 2000])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        outputs = outputs\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        result = result\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StickyNN(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super(StickyNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(82944, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2)  #num classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, train_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            val_acc += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            \n",
    "    return {\n",
    "        'train_loss': train_loss / len(train_loader.dataset),\n",
    "        'val_loss': val_loss / len(val_loader.dataset),\n",
    "        'val_acc': val_acc / total\n",
    "    }\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, device, opt_func=torch.optim.Adam):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        result = evaluate(model,val_loader, train_loader, device)\n",
    "        model.epoch_end(epoch, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StickyNN()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6443, val_loss: 0.6271, val_acc: 0.7064\n",
      "Epoch [1], train_loss: 0.4640, val_loss: 0.4420, val_acc: 0.8624\n",
      "Epoch [2], train_loss: 0.3011, val_loss: 0.3033, val_acc: 0.9266\n",
      "Epoch [3], train_loss: 0.5513, val_loss: 0.5065, val_acc: 0.6697\n",
      "Epoch [4], train_loss: 0.3273, val_loss: 0.2817, val_acc: 0.8991\n",
      "Epoch [5], train_loss: 0.4123, val_loss: 0.3641, val_acc: 0.7156\n",
      "Epoch [6], train_loss: 0.2147, val_loss: 0.2182, val_acc: 0.9174\n",
      "Epoch [7], train_loss: 0.2449, val_loss: 0.2533, val_acc: 0.9083\n",
      "Epoch [8], train_loss: 0.1765, val_loss: 0.2001, val_acc: 0.9266\n",
      "Epoch [9], train_loss: 0.1668, val_loss: 0.1814, val_acc: 0.9083\n",
      "Epoch [10], train_loss: 0.1481, val_loss: 0.1768, val_acc: 0.9174\n",
      "Epoch [11], train_loss: 0.1433, val_loss: 0.1623, val_acc: 0.9266\n",
      "Epoch [12], train_loss: 0.1272, val_loss: 0.1604, val_acc: 0.9266\n",
      "Epoch [13], train_loss: 0.1403, val_loss: 0.1755, val_acc: 0.9266\n",
      "Epoch [14], train_loss: 0.1510, val_loss: 0.1643, val_acc: 0.9358\n",
      "Epoch [15], train_loss: 0.2141, val_loss: 0.2029, val_acc: 0.9266\n",
      "Epoch [16], train_loss: 0.1399, val_loss: 0.1750, val_acc: 0.9266\n",
      "Epoch [17], train_loss: 0.1310, val_loss: 0.2000, val_acc: 0.9174\n",
      "Epoch [18], train_loss: 0.1165, val_loss: 0.1812, val_acc: 0.9174\n",
      "Epoch [19], train_loss: 0.1070, val_loss: 0.1732, val_acc: 0.9174\n",
      "Epoch [20], train_loss: 0.1040, val_loss: 0.1757, val_acc: 0.9358\n",
      "Epoch [21], train_loss: 0.1191, val_loss: 0.1687, val_acc: 0.9266\n",
      "Epoch [22], train_loss: 0.0967, val_loss: 0.1844, val_acc: 0.9358\n",
      "Epoch [23], train_loss: 0.5902, val_loss: 0.5339, val_acc: 0.7156\n",
      "Epoch [24], train_loss: 0.1141, val_loss: 0.1718, val_acc: 0.9450\n",
      "Epoch [25], train_loss: 0.1115, val_loss: 0.1700, val_acc: 0.9358\n",
      "Epoch [26], train_loss: 0.0883, val_loss: 0.1424, val_acc: 0.9358\n",
      "Epoch [27], train_loss: 0.1043, val_loss: 0.1468, val_acc: 0.9266\n",
      "Epoch [28], train_loss: 0.0771, val_loss: 0.1766, val_acc: 0.9266\n",
      "Epoch [29], train_loss: 0.0881, val_loss: 0.1912, val_acc: 0.9174\n",
      "Epoch [30], train_loss: 0.0683, val_loss: 0.1574, val_acc: 0.9266\n",
      "Epoch [31], train_loss: 0.1434, val_loss: 0.1780, val_acc: 0.9633\n",
      "Epoch [32], train_loss: 0.1012, val_loss: 0.2021, val_acc: 0.9358\n",
      "Epoch [33], train_loss: 0.0948, val_loss: 0.2383, val_acc: 0.9358\n",
      "Epoch [34], train_loss: 0.0461, val_loss: 0.1566, val_acc: 0.9358\n",
      "Epoch [35], train_loss: 0.0481, val_loss: 0.1424, val_acc: 0.9541\n",
      "Epoch [36], train_loss: 0.0343, val_loss: 0.1203, val_acc: 0.9541\n",
      "Epoch [37], train_loss: 0.0300, val_loss: 0.1479, val_acc: 0.9266\n",
      "Epoch [38], train_loss: 0.0313, val_loss: 0.1600, val_acc: 0.9450\n",
      "Epoch [39], train_loss: 0.0162, val_loss: 0.1769, val_acc: 0.9541\n",
      "Epoch [40], train_loss: 0.0093, val_loss: 0.2103, val_acc: 0.9450\n",
      "Epoch [41], train_loss: 0.0272, val_loss: 0.2282, val_acc: 0.9450\n",
      "Epoch [42], train_loss: 0.0291, val_loss: 0.1397, val_acc: 0.9450\n",
      "Epoch [43], train_loss: 0.0204, val_loss: 0.1587, val_acc: 0.9450\n",
      "Epoch [44], train_loss: 0.0126, val_loss: 0.1242, val_acc: 0.9633\n",
      "Epoch [45], train_loss: 0.0143, val_loss: 0.1567, val_acc: 0.9450\n",
      "Epoch [46], train_loss: 0.0090, val_loss: 0.1263, val_acc: 0.9541\n",
      "Epoch [47], train_loss: 0.0019, val_loss: 0.1567, val_acc: 0.9541\n",
      "Epoch [48], train_loss: 0.0022, val_loss: 0.2747, val_acc: 0.9541\n",
      "Epoch [49], train_loss: 0.0005, val_loss: 0.1990, val_acc: 0.9633\n",
      "Epoch [50], train_loss: 0.0003, val_loss: 0.2081, val_acc: 0.9541\n",
      "Epoch [51], train_loss: 0.0002, val_loss: 0.2373, val_acc: 0.9541\n",
      "Epoch [52], train_loss: 0.0001, val_loss: 0.2490, val_acc: 0.9541\n",
      "Epoch [53], train_loss: 0.0001, val_loss: 0.2491, val_acc: 0.9541\n",
      "Epoch [54], train_loss: 0.0001, val_loss: 0.2580, val_acc: 0.9541\n",
      "Epoch [55], train_loss: 0.0000, val_loss: 0.2640, val_acc: 0.9541\n",
      "Epoch [56], train_loss: 0.0000, val_loss: 0.2695, val_acc: 0.9541\n",
      "Epoch [57], train_loss: 0.0000, val_loss: 0.2760, val_acc: 0.9541\n",
      "Epoch [58], train_loss: 0.0000, val_loss: 0.2819, val_acc: 0.9541\n",
      "Epoch [59], train_loss: 0.0000, val_loss: 0.2835, val_acc: 0.9541\n",
      "Epoch [60], train_loss: 0.0000, val_loss: 0.2911, val_acc: 0.9541\n",
      "Epoch [61], train_loss: 0.0000, val_loss: 0.2930, val_acc: 0.9541\n",
      "Epoch [62], train_loss: 0.0000, val_loss: 0.2980, val_acc: 0.9541\n",
      "Epoch [63], train_loss: 0.0000, val_loss: 0.3017, val_acc: 0.9541\n",
      "Epoch [64], train_loss: 0.0000, val_loss: 0.3056, val_acc: 0.9541\n",
      "Epoch [65], train_loss: 0.0000, val_loss: 0.3098, val_acc: 0.9541\n",
      "Epoch [66], train_loss: 0.0000, val_loss: 0.3138, val_acc: 0.9541\n",
      "Epoch [67], train_loss: 0.0000, val_loss: 0.3191, val_acc: 0.9541\n",
      "Epoch [68], train_loss: 0.0000, val_loss: 0.3296, val_acc: 0.9541\n",
      "Epoch [69], train_loss: 0.0000, val_loss: 0.3364, val_acc: 0.9541\n",
      "Epoch [70], train_loss: 0.0000, val_loss: 0.3373, val_acc: 0.9541\n",
      "Epoch [71], train_loss: 0.0000, val_loss: 0.3257, val_acc: 0.9541\n",
      "Epoch [72], train_loss: 0.0000, val_loss: 0.3265, val_acc: 0.9541\n",
      "Epoch [73], train_loss: 0.0000, val_loss: 0.3208, val_acc: 0.9541\n",
      "Epoch [74], train_loss: 0.0000, val_loss: 0.3163, val_acc: 0.9541\n",
      "Epoch [75], train_loss: 0.0000, val_loss: 0.3052, val_acc: 0.9541\n",
      "Epoch [76], train_loss: 0.0000, val_loss: 0.3057, val_acc: 0.9541\n",
      "Epoch [77], train_loss: 0.0000, val_loss: 0.3062, val_acc: 0.9541\n",
      "Epoch [78], train_loss: 0.0000, val_loss: 0.3093, val_acc: 0.9541\n",
      "Epoch [79], train_loss: 0.0000, val_loss: 0.3010, val_acc: 0.9541\n",
      "Epoch [80], train_loss: 0.0000, val_loss: 0.3090, val_acc: 0.9541\n",
      "Epoch [81], train_loss: 0.0000, val_loss: 0.3007, val_acc: 0.9541\n",
      "Epoch [82], train_loss: 0.0000, val_loss: 0.3014, val_acc: 0.9541\n",
      "Epoch [83], train_loss: 0.0000, val_loss: 0.3053, val_acc: 0.9541\n",
      "Epoch [84], train_loss: 0.0000, val_loss: 0.2979, val_acc: 0.9541\n",
      "Epoch [85], train_loss: 0.0000, val_loss: 0.3055, val_acc: 0.9541\n",
      "Epoch [86], train_loss: 0.0000, val_loss: 0.2990, val_acc: 0.9541\n",
      "Epoch [87], train_loss: 0.0000, val_loss: 0.3034, val_acc: 0.9541\n",
      "Epoch [88], train_loss: 0.0000, val_loss: 0.2965, val_acc: 0.9541\n",
      "Epoch [89], train_loss: 0.0000, val_loss: 0.2974, val_acc: 0.9541\n",
      "Epoch [90], train_loss: 0.0000, val_loss: 0.3025, val_acc: 0.9541\n",
      "Epoch [91], train_loss: 0.0000, val_loss: 0.2983, val_acc: 0.9541\n",
      "Epoch [92], train_loss: 0.0000, val_loss: 0.2978, val_acc: 0.9541\n",
      "Epoch [93], train_loss: 0.0000, val_loss: 0.3016, val_acc: 0.9541\n",
      "Epoch [94], train_loss: 0.0000, val_loss: 0.2946, val_acc: 0.9541\n",
      "Epoch [95], train_loss: 0.0000, val_loss: 0.2962, val_acc: 0.9541\n",
      "Epoch [96], train_loss: 0.0000, val_loss: 0.2972, val_acc: 0.9541\n",
      "Epoch [97], train_loss: 0.0000, val_loss: 0.2936, val_acc: 0.9541\n",
      "Epoch [98], train_loss: 0.0000, val_loss: 0.2955, val_acc: 0.9541\n",
      "Epoch [99], train_loss: 0.0000, val_loss: 0.2968, val_acc: 0.9541\n",
      "Epoch [100], train_loss: 0.0000, val_loss: 0.2926, val_acc: 0.9541\n",
      "Epoch [101], train_loss: 0.0000, val_loss: 0.2925, val_acc: 0.9541\n",
      "Epoch [102], train_loss: 0.0000, val_loss: 0.2930, val_acc: 0.9541\n",
      "Epoch [103], train_loss: 0.0000, val_loss: 0.2932, val_acc: 0.9541\n",
      "Epoch [104], train_loss: 0.0000, val_loss: 0.2932, val_acc: 0.9541\n",
      "Epoch [105], train_loss: 0.0000, val_loss: 0.2937, val_acc: 0.9541\n",
      "Epoch [106], train_loss: 0.0000, val_loss: 0.2952, val_acc: 0.9541\n",
      "Epoch [107], train_loss: 0.0000, val_loss: 0.2947, val_acc: 0.9541\n",
      "Epoch [108], train_loss: 0.0000, val_loss: 0.2922, val_acc: 0.9541\n",
      "Epoch [109], train_loss: 0.0000, val_loss: 0.2944, val_acc: 0.9541\n",
      "Epoch [110], train_loss: 0.0000, val_loss: 0.2918, val_acc: 0.9541\n",
      "Epoch [111], train_loss: 0.0000, val_loss: 0.2910, val_acc: 0.9541\n",
      "Epoch [112], train_loss: 0.0000, val_loss: 0.2920, val_acc: 0.9541\n",
      "Epoch [113], train_loss: 0.0000, val_loss: 0.2944, val_acc: 0.9541\n",
      "Epoch [114], train_loss: 0.0000, val_loss: 0.2935, val_acc: 0.9541\n",
      "Epoch [115], train_loss: 0.0000, val_loss: 0.2907, val_acc: 0.9541\n",
      "Epoch [116], train_loss: 0.0000, val_loss: 0.2914, val_acc: 0.9541\n",
      "Epoch [117], train_loss: 0.0000, val_loss: 0.2936, val_acc: 0.9541\n",
      "Epoch [118], train_loss: 0.0000, val_loss: 0.2890, val_acc: 0.9541\n",
      "Epoch [119], train_loss: 0.0000, val_loss: 0.2924, val_acc: 0.9541\n"
     ]
    }
   ],
   "source": [
    "fit(num_epochs, lr, model, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            test_acc += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "    return {\n",
    "        'test_loss': test_loss / len(test_loader.dataset),\n",
    "        'test_acc': test_acc / total\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 6.350018360700266, 'test_acc': 0.6414342629482072}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data_dir = \"./public/normal_dataset\"\n",
    "normal_dataset = ImageFolder(data_dir, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "novel_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "normal_dataloader = DataLoader(normal_dataset, batch_size=batch_size, shuffle=True)\n",
    "novel_dataloader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # 150x150x3 -> 75x75x16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # 75x75x16 -> 38x38x32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 38x38x32 -> 19x19x64\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 19x19x64 -> 38x38x32\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 38x38x32 -> 76x76x16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),   # 76x76x16 -> 152x152x3\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        # Adjust the output to match the input size\n",
    "        x = x[:, :, :150, :150]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_autoencoder(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "def eval_autoencoder(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_autoencoder(model, dataloader, normal_threshold, novel_threshold, num_epochs=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            if loss.item() < normal_threshold:\n",
    "                loss.backward()\n",
    "            elif loss.item() > novel_threshold:\n",
    "                (-loss).backward()  \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "autoencoder = autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120], Loss: 0.2731\n",
      "Epoch [2/120], Loss: 0.0220\n",
      "Epoch [3/120], Loss: 0.0088\n",
      "Epoch [4/120], Loss: 0.0059\n",
      "Epoch [5/120], Loss: 0.0050\n",
      "Epoch [6/120], Loss: 0.0046\n",
      "Epoch [7/120], Loss: 0.0043\n",
      "Epoch [8/120], Loss: 0.0040\n",
      "Epoch [9/120], Loss: 0.0038\n",
      "Epoch [10/120], Loss: 0.0036\n",
      "Epoch [11/120], Loss: 0.0035\n",
      "Epoch [12/120], Loss: 0.0033\n",
      "Epoch [13/120], Loss: 0.0032\n",
      "Epoch [14/120], Loss: 0.0031\n",
      "Epoch [15/120], Loss: 0.0028\n",
      "Epoch [16/120], Loss: 0.0026\n",
      "Epoch [17/120], Loss: 0.0023\n",
      "Epoch [18/120], Loss: 0.0022\n",
      "Epoch [19/120], Loss: 0.0020\n",
      "Epoch [20/120], Loss: 0.0019\n",
      "Epoch [21/120], Loss: 0.0018\n",
      "Epoch [22/120], Loss: 0.0018\n",
      "Epoch [23/120], Loss: 0.0017\n",
      "Epoch [24/120], Loss: 0.0018\n",
      "Epoch [25/120], Loss: 0.0018\n",
      "Epoch [26/120], Loss: 0.0016\n",
      "Epoch [27/120], Loss: 0.0015\n",
      "Epoch [28/120], Loss: 0.0015\n",
      "Epoch [29/120], Loss: 0.0014\n",
      "Epoch [30/120], Loss: 0.0014\n",
      "Epoch [31/120], Loss: 0.0020\n",
      "Epoch [32/120], Loss: 0.0014\n",
      "Epoch [33/120], Loss: 0.0013\n",
      "Epoch [34/120], Loss: 0.0013\n",
      "Epoch [35/120], Loss: 0.0013\n",
      "Epoch [36/120], Loss: 0.0012\n",
      "Epoch [37/120], Loss: 0.0012\n",
      "Epoch [38/120], Loss: 0.0012\n",
      "Epoch [39/120], Loss: 0.0012\n",
      "Epoch [40/120], Loss: 0.0011\n",
      "Epoch [41/120], Loss: 0.0011\n",
      "Epoch [42/120], Loss: 0.0011\n",
      "Epoch [43/120], Loss: 0.0011\n",
      "Epoch [44/120], Loss: 0.0011\n",
      "Epoch [45/120], Loss: 0.0011\n",
      "Epoch [46/120], Loss: 0.0013\n",
      "Epoch [47/120], Loss: 0.0011\n",
      "Epoch [48/120], Loss: 0.0010\n",
      "Epoch [49/120], Loss: 0.0010\n",
      "Epoch [50/120], Loss: 0.0010\n",
      "Epoch [51/120], Loss: 0.0009\n",
      "Epoch [52/120], Loss: 0.0015\n",
      "Epoch [53/120], Loss: 0.0010\n",
      "Epoch [54/120], Loss: 0.0010\n",
      "Epoch [55/120], Loss: 0.0009\n",
      "Epoch [56/120], Loss: 0.0009\n",
      "Epoch [57/120], Loss: 0.0009\n",
      "Epoch [58/120], Loss: 0.0009\n",
      "Epoch [59/120], Loss: 0.0009\n",
      "Epoch [60/120], Loss: 0.0009\n",
      "Epoch [61/120], Loss: 0.0009\n",
      "Epoch [62/120], Loss: 0.0009\n",
      "Epoch [63/120], Loss: 0.0009\n",
      "Epoch [64/120], Loss: 0.0009\n",
      "Epoch [65/120], Loss: 0.0009\n",
      "Epoch [66/120], Loss: 0.0012\n",
      "Epoch [67/120], Loss: 0.0010\n",
      "Epoch [68/120], Loss: 0.0008\n",
      "Epoch [69/120], Loss: 0.0008\n",
      "Epoch [70/120], Loss: 0.0008\n",
      "Epoch [71/120], Loss: 0.0008\n",
      "Epoch [72/120], Loss: 0.0008\n",
      "Epoch [73/120], Loss: 0.0008\n",
      "Epoch [74/120], Loss: 0.0008\n",
      "Epoch [75/120], Loss: 0.0009\n",
      "Epoch [76/120], Loss: 0.0008\n",
      "Epoch [77/120], Loss: 0.0007\n",
      "Epoch [78/120], Loss: 0.0007\n",
      "Epoch [79/120], Loss: 0.0007\n",
      "Epoch [80/120], Loss: 0.0008\n",
      "Epoch [81/120], Loss: 0.0008\n",
      "Epoch [82/120], Loss: 0.0008\n",
      "Epoch [83/120], Loss: 0.0007\n",
      "Epoch [84/120], Loss: 0.0007\n",
      "Epoch [85/120], Loss: 0.0007\n",
      "Epoch [86/120], Loss: 0.0007\n",
      "Epoch [87/120], Loss: 0.0007\n",
      "Epoch [88/120], Loss: 0.0007\n",
      "Epoch [89/120], Loss: 0.0007\n",
      "Epoch [90/120], Loss: 0.0007\n",
      "Epoch [91/120], Loss: 0.0007\n",
      "Epoch [92/120], Loss: 0.0006\n",
      "Epoch [93/120], Loss: 0.0006\n",
      "Epoch [94/120], Loss: 0.0006\n",
      "Epoch [95/120], Loss: 0.0006\n",
      "Epoch [96/120], Loss: 0.0011\n",
      "Epoch [97/120], Loss: 0.0008\n",
      "Epoch [98/120], Loss: 0.0007\n",
      "Epoch [99/120], Loss: 0.0007\n",
      "Epoch [100/120], Loss: 0.0006\n",
      "Epoch [101/120], Loss: 0.0006\n",
      "Epoch [102/120], Loss: 0.0006\n",
      "Epoch [103/120], Loss: 0.0007\n",
      "Epoch [104/120], Loss: 0.0006\n",
      "Epoch [105/120], Loss: 0.0006\n",
      "Epoch [106/120], Loss: 0.0006\n",
      "Epoch [107/120], Loss: 0.0011\n",
      "Epoch [108/120], Loss: 0.0007\n",
      "Epoch [109/120], Loss: 0.0006\n",
      "Epoch [110/120], Loss: 0.0006\n",
      "Epoch [111/120], Loss: 0.0006\n",
      "Epoch [112/120], Loss: 0.0006\n",
      "Epoch [113/120], Loss: 0.0006\n",
      "Epoch [114/120], Loss: 0.0006\n",
      "Epoch [115/120], Loss: 0.0006\n",
      "Epoch [116/120], Loss: 0.0006\n",
      "Epoch [117/120], Loss: 0.0006\n",
      "Epoch [118/120], Loss: 0.0006\n",
      "Epoch [119/120], Loss: 0.0006\n",
      "Epoch [120/120], Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "fit_autoencoder(autoencoder, normal_dataloader, nn.MSELoss(), torch.optim.Adam(autoencoder.parameters()), 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005630059985912813"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_autoencoder(autoencoder, normal_dataloader, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_threshold = 0.1  \n",
    "novel_threshold = 0.5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120], Loss: 0.0012\n",
      "Epoch [2/120], Loss: 0.0006\n",
      "Epoch [3/120], Loss: 0.0005\n",
      "Epoch [4/120], Loss: 0.0005\n",
      "Epoch [5/120], Loss: 0.0005\n",
      "Epoch [6/120], Loss: 0.0005\n",
      "Epoch [7/120], Loss: 0.0005\n",
      "Epoch [8/120], Loss: 0.0015\n",
      "Epoch [9/120], Loss: 0.0006\n",
      "Epoch [10/120], Loss: 0.0006\n",
      "Epoch [11/120], Loss: 0.0006\n",
      "Epoch [12/120], Loss: 0.0006\n",
      "Epoch [13/120], Loss: 0.0006\n",
      "Epoch [14/120], Loss: 0.0005\n",
      "Epoch [15/120], Loss: 0.0005\n",
      "Epoch [16/120], Loss: 0.0005\n",
      "Epoch [17/120], Loss: 0.0005\n",
      "Epoch [18/120], Loss: 0.0005\n",
      "Epoch [19/120], Loss: 0.0005\n",
      "Epoch [20/120], Loss: 0.0005\n",
      "Epoch [21/120], Loss: 0.0005\n",
      "Epoch [22/120], Loss: 0.0005\n",
      "Epoch [23/120], Loss: 0.0005\n",
      "Epoch [24/120], Loss: 0.0005\n",
      "Epoch [25/120], Loss: 0.0005\n",
      "Epoch [26/120], Loss: 0.0005\n",
      "Epoch [27/120], Loss: 0.0005\n",
      "Epoch [28/120], Loss: 0.0005\n",
      "Epoch [29/120], Loss: 0.0006\n",
      "Epoch [30/120], Loss: 0.0005\n",
      "Epoch [31/120], Loss: 0.0005\n",
      "Epoch [32/120], Loss: 0.0005\n",
      "Epoch [33/120], Loss: 0.0005\n",
      "Epoch [34/120], Loss: 0.0005\n",
      "Epoch [35/120], Loss: 0.0005\n",
      "Epoch [36/120], Loss: 0.0006\n",
      "Epoch [37/120], Loss: 0.0011\n",
      "Epoch [38/120], Loss: 0.0005\n",
      "Epoch [39/120], Loss: 0.0005\n",
      "Epoch [40/120], Loss: 0.0005\n",
      "Epoch [41/120], Loss: 0.0005\n",
      "Epoch [42/120], Loss: 0.0005\n",
      "Epoch [43/120], Loss: 0.0005\n",
      "Epoch [44/120], Loss: 0.0005\n",
      "Epoch [45/120], Loss: 0.0005\n",
      "Epoch [46/120], Loss: 0.0005\n",
      "Epoch [47/120], Loss: 0.0005\n",
      "Epoch [48/120], Loss: 0.0005\n",
      "Epoch [49/120], Loss: 0.0005\n",
      "Epoch [50/120], Loss: 0.0005\n",
      "Epoch [51/120], Loss: 0.0007\n",
      "Epoch [52/120], Loss: 0.0005\n",
      "Epoch [53/120], Loss: 0.0005\n",
      "Epoch [54/120], Loss: 0.0004\n",
      "Epoch [55/120], Loss: 0.0005\n",
      "Epoch [56/120], Loss: 0.0004\n",
      "Epoch [57/120], Loss: 0.0004\n",
      "Epoch [58/120], Loss: 0.0005\n",
      "Epoch [59/120], Loss: 0.0007\n",
      "Epoch [60/120], Loss: 0.0005\n",
      "Epoch [61/120], Loss: 0.0004\n",
      "Epoch [62/120], Loss: 0.0005\n",
      "Epoch [63/120], Loss: 0.0004\n",
      "Epoch [64/120], Loss: 0.0004\n",
      "Epoch [65/120], Loss: 0.0004\n",
      "Epoch [66/120], Loss: 0.0004\n",
      "Epoch [67/120], Loss: 0.0004\n",
      "Epoch [68/120], Loss: 0.0004\n",
      "Epoch [69/120], Loss: 0.0004\n",
      "Epoch [70/120], Loss: 0.0004\n",
      "Epoch [71/120], Loss: 0.0004\n",
      "Epoch [72/120], Loss: 0.0005\n",
      "Epoch [73/120], Loss: 0.0004\n",
      "Epoch [74/120], Loss: 0.0004\n",
      "Epoch [75/120], Loss: 0.0006\n",
      "Epoch [76/120], Loss: 0.0010\n",
      "Epoch [77/120], Loss: 0.0005\n",
      "Epoch [78/120], Loss: 0.0004\n",
      "Epoch [79/120], Loss: 0.0004\n",
      "Epoch [80/120], Loss: 0.0004\n",
      "Epoch [81/120], Loss: 0.0004\n",
      "Epoch [82/120], Loss: 0.0008\n",
      "Epoch [83/120], Loss: 0.0006\n",
      "Epoch [84/120], Loss: 0.0004\n",
      "Epoch [85/120], Loss: 0.0004\n",
      "Epoch [86/120], Loss: 0.0004\n",
      "Epoch [87/120], Loss: 0.0004\n",
      "Epoch [88/120], Loss: 0.0004\n",
      "Epoch [89/120], Loss: 0.0004\n",
      "Epoch [90/120], Loss: 0.0004\n",
      "Epoch [91/120], Loss: 0.0004\n",
      "Epoch [92/120], Loss: 0.0004\n",
      "Epoch [93/120], Loss: 0.0004\n",
      "Epoch [94/120], Loss: 0.0004\n",
      "Epoch [95/120], Loss: 0.0005\n",
      "Epoch [96/120], Loss: 0.0007\n",
      "Epoch [97/120], Loss: 0.0005\n",
      "Epoch [98/120], Loss: 0.0004\n",
      "Epoch [99/120], Loss: 0.0004\n",
      "Epoch [100/120], Loss: 0.0004\n",
      "Epoch [101/120], Loss: 0.0004\n",
      "Epoch [102/120], Loss: 0.0004\n",
      "Epoch [103/120], Loss: 0.0004\n",
      "Epoch [104/120], Loss: 0.0004\n",
      "Epoch [105/120], Loss: 0.0004\n",
      "Epoch [106/120], Loss: 0.0004\n",
      "Epoch [107/120], Loss: 0.0004\n",
      "Epoch [108/120], Loss: 0.0004\n",
      "Epoch [109/120], Loss: 0.0004\n",
      "Epoch [110/120], Loss: 0.0004\n",
      "Epoch [111/120], Loss: 0.0005\n",
      "Epoch [112/120], Loss: 0.0005\n",
      "Epoch [113/120], Loss: 0.0004\n",
      "Epoch [114/120], Loss: 0.0004\n",
      "Epoch [115/120], Loss: 0.0004\n",
      "Epoch [116/120], Loss: 0.0004\n",
      "Epoch [117/120], Loss: 0.0004\n",
      "Epoch [118/120], Loss: 0.0005\n",
      "Epoch [119/120], Loss: 0.0004\n",
      "Epoch [120/120], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "fine_tune_autoencoder(autoencoder, normal_dataloader, normal_threshold, novel_threshold, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00022977744387928396"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_autoencoder(autoencoder, novel_dataloader, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_image(image_path):\n",
    "   img = Image.open(image_path)\n",
    "   transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "   ])\n",
    "   if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "   img_tf = transform(img).float().unsqueeze(0)\n",
    "   return img_tf\n",
    "\n",
    "def detect_novelty(image, autoencoder_model, æ_threshold = 0.5):\n",
    "    image = image.to(device)\n",
    "    encoder_output = autoencoder_model(image)\n",
    "    ae_loss = nn.MSELoss()(encoder_output, image)\n",
    "\n",
    "\n",
    "    if ae_loss > æ_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def detect(image, cnn_model, autoencoder_model, train_data, æ_threshold = 0.5):\n",
    "    novel = detect_novelty(image, autoencoder_model, æ_threshold)\n",
    "    if novel:\n",
    "        return \"Can't recognise it, must be smth different\"\n",
    "    else: \n",
    "        with torch.no_grad():\n",
    "            model.eval() \n",
    "            image = image.to(device) \n",
    "            output =cnn_model(image)\n",
    "            index = output.data.cpu().numpy().argmax()\n",
    "            classes = train_data.classes\n",
    "            class_name = classes[index]\n",
    "            return f\"Guess this is a {class_name}!\"\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess this is a leg!\n"
     ]
    }
   ],
   "source": [
    "img = pre_image(\"./public/images.jpg\")\n",
    "message = detect(img,model,autoencoder,dataset)\n",
    "print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CNN = './models/StickyCNN.pth'\n",
    "torch.save(model.state_dict(), PATH_CNN)\n",
    "PATH_AE = './models/Autoencoder.pth'\n",
    "torch.save(model.state_dict(), PATH_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StickyNN(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=82944, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel = StickyNN()\n",
    "loadedModel.load_state_dict(torch.load('./models/StickyCNN.pth'))\n",
    "loadedModel.to(device)\n",
    "loadedModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess this is a leg!\n"
     ]
    }
   ],
   "source": [
    "img = pre_image(\"./public/images.jpg\")\n",
    "message = detect(img,loadedModel,autoencoder,dataset)\n",
    "print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
